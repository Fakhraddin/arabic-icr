\documentclass[10pt, conference, compsocconf]{IEEEtran}
\usepackage[nocompress]{cite}
\usepackage[font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{graphicx}
\usepackage{arabtex}
\usepackage[cmex10]{amsmath}

\begin{document}

\title{Fast Classification of Handwritten On-line Arabic Characters}

\author{\IEEEauthorblockN{George Kour}
\IEEEauthorblockA{Faculty of Engineering\\
Tel-Aviv University\\
Tel-Aviv Jaffa, Israel\\
Email: georgeko@post.tau.ac.il}
\and
\IEEEauthorblockN{Raid Saabne}
\IEEEauthorblockA{Department of Computer Science\\
Tel Aviv-Yaffo Academic Collage, Israel\\
Triangle R\&D Center, Kafr Qara, Israel\\
Email: saabni@cs.bgu.ac.il}
}


\maketitle

\begin{abstract}
Delaying the analysis launch until the completion of the handwritten word scribing, restricts on-line recognition techniques to meet the highly responsiveness demands expected from such systems, and prevents implementing advanced features of input typing, such as automatic word completion and real-time automatic spelling.
This paper proposes an efficient recognition of handwritten Arabic characters method, employed on a real-time words segmentation and techniques described in \cite{kour2014real}.
We use the Shape Context and the Multi Angular Descriptor to extract features from each significant point on the stroke. 
PCA and LDA based dimensionality reduction process followed by an embedding to a normed wavelet coefficients domain are used to approximate the Earth Movers Distance Metric as shape similarity function for sub linear time process of character classification. 
In the next step, results of the top ranked shapes of each predicted character are used to generate a list of candidate shapes of Arabic word parts in a filter and refine approach to enable real-time yet accurate results of recognition in a dictionary-free environment.
The system was trained and tested on characters and word parts extracted from the ADAB database, and promising accuracy and performance results were achieved.
\end{abstract}

\begin{IEEEkeywords}
Arabic character recognition; handwriting recognition; on-line script recognition; real-time handwriting segmentation;
\end{IEEEkeywords}

\section{Introduction}
The growing use of keyboard-less electronic devices, such as smart-phones and tablets, and the fact that handwriting remains the most commonly used mean for information recording and communication, gave rise to a significant increase in interest in on-line handwriting recognition.

The Arabic script is written from right to left in a semi-cursive manner in both printed and handwritten. 
Most letters are written in four different letter shapes depending on their position in a word.
Among the basic letters, six are dis-connective which do not connect to the following letter and have only two shapes each. 
The presence of dis-connective letters interrupts the continuity of the graphic form of a word and closes the \emph{word part} (WP).

The cursiveness of the Arabic script, prima facie, requires delaying the launch of the recognition process until the completion of the word scribing.
However, a recognition-based segmentation approach was proposed in \cite{kour2014real}, demonstrating the feasibility of segmenting WPs while the stroke is being written.
The resulted segmentation and letters classification information can be used to significantly reduce the potential dictionary size and accelerate a later holistic recognition process.
The real-time nature of the segmentation technique, strictly requires the characters classifier to be extremely fast.

The unconstrained nature of the Arabic handwriting requires the letters classification system to be trained over a large set of samples that cover the large variation of letters forms.
Similarity measure techniques that imitate the human intuition, such as the well known \emph{dynamic time warping} (DTW) and the \emph{earth mover's distance} (EMD), are computationally expensive and may take more than a few hours to find, for a given query object, the most similar objects in a large sample set, on an average personal computer.
Thus, it is desirable to make as few as possible such distance calculations.
A well known technique for achieving this speed-up is by embedding the compared objects into a space equipped with an efficient metric which approximate the distance function in the original space.

In this paper, we propose an efficient character classifier using the EMD metric as the similarity measure. 
Linear time embedding algorithm for approximating EMD \cite{shirdhonkar2008approximate} is employed to transform the feature vectors to a normed space and facilitating the use of fast nearest neighbours search techniques.
Then, a $k$-NN classifier is used to produces a short list of candidates enabling us to apply expensive matching methods, such as DTW, yet keep sub-linear searching time.

The rest of the paper is organized is follows. 
In Section \ref{sec:related_work} we mention related works. 
The character classifier is described in Section \ref{sec:approach}. 
In Section \ref{sec:samples_collection} we describe the letter samples extraction from the ADAB database.
Demonstrating and discussing the results is done in Section \ref{sec:results_analysis}.
Summary and future direction, are presented in Section \ref{sec:summary_future_work}. 

\section{Related Work}
\label{sec:related_work}

Literature in the field of cursive handwriting recognition has established two main approaches: the analytic approach, which involves segmenting words into individual characters and then classifying each character\cite{abdulla2008off, sari2002off, Dinges2011}, and the holistic approach, which considers the global properties of the written text and recognizes the input word shape as a whole \cite{biadsy2011segmentation, saabni2009hierarchical}. 
The holistic approach, while having many advantages, requires the classifier to be trained over the entire dictionary, which is impractical for large dictionaries (containing more than 20,000 words) \cite{elanwar2012unconstrained}.

One of the earliest studies on On-line Arabic isolated character recognition was carried out by El-Wakil and Shoukry in 1989 \cite{el1989line}.
Many types of classification techniques were investigated since then, including artificial neural networks \cite{alijla2012oiahcr,ismail2012online}, decision trees \cite{ismail2012online}, Hidden Markov Models \cite{biadsy2006online} and $k$-NN \cite{elglaly2011isolated}.
It appears that currently the $k$-NN is one of the leading techniques.

EMD is a natural and intuitive metric to measure similarity between histograms. 
It has been successfully used in many fields of image matching and retrieval \cite{grauman2004fast, rubner2000earth}.
A significant speed-up can be achieved by embedding vectors from one space with an expensive distance measure into another space equipped with an easy-to-compute distance function \cite{saabni2013efficient}. 
Much work has been done on embedding the EMD metric into a normed space, usually the $l_p$ norm, in order to facilitate efficient and fast nearest neighbour extraction using indexing techniques \cite{bourgain1985lipschitz}. 
The efficient embedding of the EMD metric proposed by \cite{shirdhonkar2008approximate} to a normed space has been used in \cite{saabni2013efficient} for efficient word image retrieval.
A previous embedding technique described by \cite{indyk2003fast} was used by Grauman and Darrell \cite{grauman2004fast} for contour based shape matching.

\section{Our Approach}
\label{sec:approach}
In the presented work, we present a fast and accurate classification technique for Arabic characters.
In Figure \ref{fig:letters_classifier_learning_flow} we give a high level flow visualization of the classification system.
The classifier receives a sequence of points $S=\{p_{i}\}_{i=1}^{n}$ representing the letter trajectory and a letter position $\phi \in \{Ini, Mid, Fin, Iso\}$.
Similarly to the process performed on the training character trajectory sequences, the query sequence goes through 5 stages: preprocessing, feature extraction, embedding, dimensionality reduction and then classification. 
Eventually the classifier outputs a short list of candidates and their scoring, which indicates the perceptual similarity between the sequence and the candidates. 
Each stage will be discussed in details in the following subsections.

\begin{figure}
\centering
\includegraphics[width=1\columnwidth]{./figures/letters_classifier_learning_flow2}       
\caption{High level diagram of the classifier data flow.}
\label{fig:letters_classifier_learning_flow}
\end{figure}
 
\subsection{Preprocessing}
Digitizers tend to generate a jagged and non-uniform sampling of the trajectory scribed on their surface, therefore, preprocessing operations are usually needed to impose certain uniform structure on the data, to comply with the structure required by the subsequent parts of the system \cite{al2011online}. 
The preprocessing stage consists of three methods: \emph{normalization}, \emph{noise elimination} and \emph{re-sampling}.

Size normalization is performed to achieve a uniform size of the bounding box surrounding the pattern so that it will fit into a $[0,1]\times[0,1]$ bounding box without affecting the original aspect ratio. 

Then, the \emph{Douglas-Peucker Polyline Simplification algorithm} \cite{douglas1973algorithms} is employed for eliminating points duplication and inadequacies caused by hand vibrations. 
It reduces the number of vertices in a piecewise linear curve, given a pre-set tolerance parameter $\varepsilon$, and outputs a simplified curve, that consists of a subset of the points that defined the original curve.
In this work the tolerance parameter $\varepsilon$ was empirically set to ${1 \over 75}$.

The simplification process produces a highly angular, and non-uniform distribution of points along the stroke trajectory.
This step, using splines interpolation, aims at producing an equidistant smoothed data sequence, given a re-sampling target number of points $R$, which was set to 40. 
Given a stroke $S=\{(x_i,y_i)\}_{i=1}^{n}$, let $f_{x}(d)$ and $f_{y}(d)$ be the quadratic piecewise interpolation functions of $\{x_i\}_{i=1}^{n}$ and $\{y_i\}_{i=1}^{n}$, respectively. 
$f_{x}(d)$ and $f_{y}(d)$ are functions of the coordinate values with respect to the arc-length distance from the pattern's starting point. 
Let $t_i=i\frac{L}{R}$ for $i=0,...,R$ where L is the arc-length of the pattern.
The re-sampled sequence is given as follows:
\begin{equation}
\widehat{S}=\{(f_x(t_i),f_y(t_i))\}_{i=1}^{R}
\end{equation}

Figure \ref{fig:before_after_preprocessing} visually demonstrates the resulting sequence after applying each step in the preprocessing stage on a trajectory sequence of the letter \RL{b}. 

\begin{figure}
	\centering
        \subfloat[]{
            \label{fig:preprocessing_orig}
            \includegraphics[width=0.4\columnwidth]{./figures/preprocessing_orig}
        }
        \subfloat[]{
           \label{fig:preprocessing_norm}
           \includegraphics[width=0.4\columnwidth]{./figures/preprocessing_norm}
        }  \\
        \subfloat[]{
            \label{fig:preprocessing_simpl}
            \includegraphics[width=0.4\columnwidth]{./figures/preprocessing_simpl}
        }
        \subfloat[]{
           \label{fig:preprocessing_resamp}
           \includegraphics[width=0.4\columnwidth]{./figures/preprocessing_resamp}
        }       
    \caption{A sample of the letter \RL{b} before preprocessing (a); after normalization (b); after noise elimination (c) and after re-sampling (d).}
   \label{fig:before_after_preprocessing}
\end{figure}

\subsection{Feature Extraction}
Two shape descriptors were employed in this work, the \emph{Shape Context} \cite{belongie2002shape} and the \emph{Multi Angular Descriptor} (MAD) \cite{saabni2013multi}.
The shape context descriptor can be considered as a point matching approach intended to be a way of describing shapes that allow measuring shapes similarity. 
It has been proved to be an efficient feature for shapes matching.

MAD captures the angular view to multi resolution rings in different heights. 
The shape is treated as a two dimensional set of points and the different rings are upper view points from rings around the shape centroid with different sizes and heights. 
To enable scale and translation invariance, the sizes and heights of these rings are calculated using the diameter and centroid of the shape.

\subsection{The Earth Mover's Distance Embedding}
Histogram based descriptors, such as the shape context, are in many cases compared using a bin-wise dissimilarity techniques such as the Euclidean distance or the $\chi^2$ statistic.
While such dissimilarity measures can be computed very fast, they usually fail to consider local and global small variations. 
These variations, which would be perceived as minor by a human, may result in a large dissimilarity value between two histograms. 
Generally speaking, the distance between two histograms can be viewed as a special case of the well-known \emph{transportation problem}, a.k.a the Monge-Kantorovich problem \cite{rachev1985monge}.
The \emph{Earth Mover's Distance} (EMD), introduced by Rubner et al. in \cite{rubner2000earth}, is a solution to the transportation problem which was experimentally verified to capture well the perceptual notion of a difference between images \cite{grauman2004fast}.

Given two distributions, one can be seen as piles of sand and the other as a collection of holes. 
EMD measures the least work needed to fill the holes with sand taken from the piles, where a unit of work corresponds to the transporting a unit of sand from the pile to the hole depending on their distance.
The major hurdle in using EMD is its $O\left( {{N^3}\log N} \right)$ computational complexity (for an $N$-bin histogram). 
Thus, searching for similar shapes (nearest neighbours) in a large database, is almost impractical since a linear scan of the database would require computing a comparison of super-polynomial complexity for each database member against the query shape \cite{saabni2013efficient}. 

Greatly reducing the EMD calculation time can be achieved using metric approximation techniques.  
Several approximation algorithms have been proposed to speed-up the computation of EMD. 
Indyk and Thaper \cite{indyk2003fast} presented a technique for embedding the un-normed EMD metric into the $L_1$ space so that the EMD distance between the two objects is comparable to the Manhattan distance between the two points which represent the embedding of the two objects.
In a subsequent work done by Shirdhonkar and Jacobs \cite{shirdhonkar2008approximate}, the authors proposed a method for approximating the EMD between two histograms using the weighted wavelet coefficients of the difference histogram. 
It is done by calculating the $L_1$ norm of the coefficients vector of the embedding as given in Equation \ref{eq:emd_embedding}.
\begin{equation}
d(p)_{wemd}= \sum\limits_{\lambda} 2^{-j(1+n/2)}|p_{\lambda}|
\label{eq:emd_embedding}
\end{equation}
where $p$ is the n-dimensional difference histogram and $p_{\lambda}$ is the wavelet transform coefficients. 
The index $\lambda$ includes both shifts and the scale j.
Intuitively, the wavelet transform splits up the difference histogram according to scale and location where each coefficient represents the solution to the EMD subproblem. 
For a single wavelet, the mass to be moved is proportional to the volume of $|\psi_j(x)|$, i.e., to $2^{-jn/2}$ and the distance to be travelled is proportional to the span of the wavelet, i.e $2^{-j}$. The sum of all distances is an approximation to EMD, as formally defined in Equation \ref{eq:emd_embedding}. 
This can be viewed as similar to the way packages are shipped over large distances. 
The route is broken into several pieces which are large and small distances. 
Packages from nearby places are merged at the end of the short distance route piece to travel together. 
Then another merge is done of packages from the entire country to be shipped together to the destination country. 
The sum of the distances travelled is an approximation to the actual distance.

\subsection{Dimensionality Reduction}
\label{subsec:dimensionality_reduction}
Dimensionality reduction methods, aims at reducing the number of variables taken into consideration in order to avoid practical complications and performance issues. 
Ideally, the reduced representation should have a dimensionality that corresponds to the intrinsic dimensionality of the data \cite{van2009dimensionality} which is the minimum number of parameters needed to account for.
The need for employing dimensionality reduction in this work emerged from the sparse vectors produces by the EMD embedding of the feature vectors into the wavelet coefficient domain.
For instance, when we employed the shape context descriptor, the embedding produced sparse vectors in $\mathbf{R}^{3946}$. 

\emph{Principle component analysis} (PCA) followed by \emph{linear discrimination analysis} (LDA) is a commonly used technique in fields where the feature space is highly dimensional, exploiting the efficiency of PCA and the discrimination power of LDA \cite{yu2001direct,yang2003can}.
The number of principle components in the PCA to consider, can be calculated easily using the sum of the eigenvalues of the covariance matrix.
The major drawback of the PCA is that it is an unsupervised technique, namely, it does not take into consideration the labelling information of the data and heavily rely on the assumption that most information is contained in those directions where input data variance is maximum.
On the contrary, \emph{linear discrimination analysis} (LDA) \cite{fisher1936use} is a supervised technique performing dimensionality reduction while preserving as much of the class discriminatory information as possible. 
In the first step, the embedded vectors are projected into the PCA space where the target dimensionality, i.e., the number of principal components taken into consideration, is the minimal to achieve data preservation rate of 98\%.
As seen in table \ref{table:dr_dimensions_results}, the dimensionality was reduced by PCA in two orders of magnitude.

Since there is a large variation in handwritten letters in the Arabic script, the grouping different perceptual shapes in a single class would negatively affect the supervised dimensionality reduction process.
Thus, before applying LDA, each character class was partitioned into four clusters, using $L_1$-k-medoids algorithm, and for each cluster a unique sub-label was assigned. 
This sub-labeling was then used by the LDA, where the target number of dimensions was estimated using the \emph{maximum likelihood estimation} method described in \cite{levina2004maximum}.

\begin{table}
\centering
\begin{tabular}{ | c | c | c | c |}
\hline
Letter position & Number of samples & PCA & PCA+LDA\\
\hline                 
  Iso & 1372 & 29 & 7 \\ 
  \hline
  Ini & 1405 & 39 & 9 \\ 
  \hline
  Mid & 1196 & 36 & 8 \\ 
  \hline
  Fin & 1629 & 27 & 7 \\ 
  \hline
\end{tabular}
\caption{The dimensionality of the four datasets after applying PCA and PCA+LDA.}
\label{table:dr_dimensions_results} 
\end{table}

The embedding of the sample set into a normed space facilitates the usage of approximate k-NN techniques such as k-d trees \cite{bentley1975multidimensional}, and Locality Sensitive Hashing (LSH) \cite{gionis1999similarity} which have been successfully applied to a variety of fast similarity retrieval problems.
Such techniques, solve the problem of searching $k$-NN in a large set and avoid linear scan of the entire dataset.

\subsection{Candidates Scoring}
Given an unlabelled sequence $q$, the $k$-NN classifier returns a set of $k$ potential letter candidates from the sample set, $\{c\}_{i=1}^{k}$.
The scoring of the candidates is done by averaging the $L_1$ distance between the query object and the candidate in the EMD space, and the DTW distance of their feature vectors, as seen in Equation \ref{eq:candidate_scoring}.
The constrained version of DTW using the Sakoe-Chuba Band \cite{sakoe1978dynamic} was used since it has achieved better results the basic version of DTW.
\begin{equation}
Scoring(c_i,q)=\frac{\|E(c_i)-E(q)\|_1+DTW_{cons.}(c_i,q)}{2}
\label{eq:candidate_scoring}
\end{equation}
where $E(\cdot)$ denotes the vector produced from the the process described earlier, i.e., preprocessing followed feature extraction, embedding into the coefficient wavelet domain, and finally, dimensionality reduction.


\section{Samples Collection}
\label{sec:samples_collection}
The character samples used in this work were extracted from the ADAB database,\cite{el2009icdar}, which is a de-facto standard in the field of on-line Arabic handwriting recognition. 
It is freely available and consists of more than 20k Arabic handwritten words (937 Tunisian town/village names) scribed by more than 170 different writers. 
A manual segmentation of the samples in the database was needed to extract the letter samples.
Using a public and not a self collected samples gives our results a further firmness.
We have used the following set distributed as follows: 1404 initial form samples, 1195 medial, 1628 final and 1371 isolated letter samples. 
The extracted set was used for both training the system and testing the performance of the classifier as detailed in the subsequent section.

\section{Results and Analysis}
\label{sec:results_analysis}

The system was implemented and tested using the Matlab environment.
The accuracy and the performance of the classifier was measured using 10-fold cross-validation method.
We have noted that although the candidates scoring according to the formula given in Equation \ref{eq:candidate_scoring} has raised the correct segmentation rate significantly, it had a limited effect on the classification accuracy. 
Thus, in the following experiments, the candidates scoring was not taken into consideration, i.e., the candidates were ranked solely according to the distance in the EMD space.

We have implemented a series of experiments aiming at quantifying the contribution and importance of each stage to the classification process.
The three main stages in the classifier are: 1. the feature extraction stage; 2. the EMD embedding; and 3. the dimensionality reduction stage.
Table \ref{table:dr_embedding_comparison} shows the results of three configurations, all using shape context as the shape descriptor.
In the first configuration, named Baseline, we evaluate the classifier as presented in this work.
In the second (No DR) and the third (No Embedding) configurations, dimensionality reduction and embedding were skipped, respectively,
For each configuration, the classifier performance was measured in terms of accuracy, by calculating the \emph{Correct Classification Rate} (CCR), and running time.
It can be noted that the dimensionality reduction stage did not affect the system performance in terms of accuracy but improved the time performance significantly.
Using k-d tree, due to its known sensitivity to high dimensional data, would result in lager performance degradation, if the dimensionality reduction stage is skipped.
In addition, it can be noted that in the third configuration, the accuracy is higher than in the Baseline configuration, when considering the top candidate. 
However, when the top three candidates are considered, the embedding performs better.
Thus, we can conclude that the candidates list obtained in the Baseline is more qualitative compared to the list obtained when the nearest neighbours is applied in the shape context space directly.

\begin{table}
\centering
\begin{tabular}{ | c | c | c | c |}
  \hline
  Experiment  & CCR [Top 1]  & CCR [Top 3] & Time [ms]\\
  \hline                 
  Baseline & 86\% & 95\% & 0.22 \\
  \hline
  No DR & 86\% & 95\% & 6.2 \\ 
  \hline
  No embedding & 90\% &  90\% & 0.41 \\
  \hline
\end{tabular}
\caption{CCR and time performance of the different configuration. 
The First row shows the results of the proposed classifier (Baseline) compared to the classifier without the Dimensionality reduction (No Dr) step and without the embedding (No Embedding).}
\label{table:dr_embedding_comparison} 
\end{table}

The results shown in Table \ref{table:features_comparison} compare the performance of the shape context and the MAD descriptors to the results obtained when none is used.

\begin{table}
\centering
\begin{tabular}{ | c | c | c |}
\hline
	Shape Descriptor  & CCR [Top 1]  & CCR [Top 3] \\
	\hline 
	Shape Context & 86\% & 95\%  \\                
  	\hline
  	MAD & 76\% & 78\% \\
  	\hline
  	None & 86\% &  87\% \\
  	\hline
\end{tabular}
\caption{Comparing the correct classification rate of the different feature extraction techniques.}
\label{table:features_comparison} 
\end{table}

The segmentation and classification information obtained by the real-time segmentation system described in \cite{kour2014real}, can be used to significantly reduce the potential dictionary size and accelerate a later holistic recognition process.
In the following experiment we evaluate the claimed advantage.
Following the results of the character segmentation and recognition process, the $10$ top scored candidates of each character are recorded. 
In the next step these shapes are used to generate a complete list of all possible shape concatenation of the retrieved candidates. 
The shapes of each candidate character many times contain different shapes of the same letter producing multiple shapes of the same WP. 
The cardinality of the generated list is $10^p$ where p is the predicted number of characters in the PAW using the segmentation process. 
Because of large the large size of the generated list, which may exceed $100,000$ shapes, the same previous process of embedding and dimensionality reduction is used to generate a short list of candidates. 
The short list of candidates in the next step is matched against the queried WP using the original expensive and more accurate matching process. 
Using the proposed approach the $10$ top results yield a $98.1\%$ recognition rate. 
The recognition rate of the first top candidate dropped down to $90.8\%$. 
Using a voting process within the top five results gave a $94.7\%$ recognition rate. 

Analysing the failures of the misclassified samples shows that most recognition errors occur as a result of misclassifying only one character. 
In most cases the misclassified character is confused with a character with a very similar shape, which can be corrected using information retrieved by the associated additional stroke, such as the case of the letter \RL{-l} and the letter \RL{-n} in their handwritten form.

\section{Summary and Future Work}
\label{sec:summary_future_work}
In this paper, we propose a high-performance approach for Arabic handwritten characters classification.
Shape context and MAD feature sets were evaluated. 
Using an efficient EMD embedding into the wavelet coefficient domain, the feature vectors were translated into the $L_1$, facilitating fast computation of the EMD approximation by calculating the Manhattan distance between the embedded vectors.
Fast retrieval of the $k$ nearest neighbours of a given query character was then possible.
A combination of PCA and LDA was employed to reduce the dimensionality of the sparse vectors generated by the embedding function, and DTW was used to refine the similarity scoring of the candidates.
We have evaluated the contribution of each stage in the classification flow to the performance and accuracy of the system.

In a future work we will expand the classifier to consider the additional strokes to improve the discrimination power of the classifier. 
We believe that this will significantly improve the segmentation and classification results.
In addition, we will expand the classifier to handle characters written in multiple strokes.

\section*{Acknowledgment}
We would like to thank Professor Dana Ron, from the Tel-Aviv University, for her valuable help in this research. 
This work has been supported in part by the German Research Foundation under grant no. FI 1494/3-2.

%\linespread{0.88}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibliography}

\end{document}



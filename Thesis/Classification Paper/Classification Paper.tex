\documentclass[10pt, conference, compsocconf]{IEEEtran}
\usepackage[nocompress]{cite}
\usepackage[font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{graphicx}
\usepackage{arabtex}
\usepackage[cmex10]{amsmath}

%packages that were not copied from the IEEE template - need to check if it is valid to add them
\usepackage{amssymb}
\usepackage[linesnumbered]{algorithm2e}

%\usepackage{etoolbox}
%\newtoggle{edit-mode}
%\togglefalse{edit-mode}  
%\toggletrue{edit-mode}


\begin{document}

\title{Fast Classification of Handwritten On-line Arabic Characters}

\author{\IEEEauthorblockN{George Kour}
\IEEEauthorblockA{Faculty of Engineering\\
Tel-Aviv University\\
Tel-Aviv Jaffa, Israel\\
Email: georgeko@post.tau.ac.il}
\and
\IEEEauthorblockN{Raid Saabne}
\IEEEauthorblockA{Department of Computer Science\\
Tel Aviv-Yaffo Academic Collage, Israel\\
Triangle R\&D Center, Kafr Qara, Israel\\
Email: saabni@cs.bgu.ac.il}
}

\maketitle

\begin{abstract}
Delaying the launch of the handwriting recognition process until the completion of the word scribing is an obstacle preventing on-line recognition technique to meet the highly responsiveness and real-time demands.
In addition, being able to recognize script while being written facilitates implementing advanced feature such as word completion.
This paper proposes a fast classification of individual handwritten Arabic characters technique used by the real-time recognition based segmentation system proposed in \cite{kour2014real}.
The system was trained and tested on characters extracted from the ADAB database, and promising accuracy and performance results were obtained.
\end{abstract}

\begin{IEEEkeywords}
Arabic character recognition; handwriting recognition; on-line script recognition; 
\end{IEEEkeywords}

%The set of limitations
%\begin{enumerate}
%\item No additional strokes are taken into considerations.
%\item letters written in the same stroke.
%\end{enumerate}

\section{Introduction}
The growing use of keyboard-less electronic devices, such as smart-phones and tablets, and the fact that handwriting remains the most commonly used mean for information recording and communication, gave rise to a significant increase in interest in On-line handwriting recognition.

The Arabic script is written from right to left in a semi-cursive manner in both printed and handwritten. 
Most letters are written in four different letter shapes depending on their position in a word.
Among the basic letters, six are Dis-connective which do not connect to the following letter and have only two shapes each. 
The presence of dis-connective letters interrupts the continuity of the graphic form of a word and closes the \emph{word-part} (WP).

The cursiveness of the Arabic script, prima facie, requires delaying the launch of the recognition process until the completion of the word scribing.
However, a recognition-based segmentation approach was proposed in \cite{kour2014real}, demonstrating the feasibility of segmenting word-parts while the stroke is being written.
The real-time nature of the segmentation technique, strictly requires the characters classifier to be extremely fast.

The unconstrained nature of the Arabic handwriting requires the letters classification system to be trained over a large set of samples that cover the large variation of letters forms.
Similarity measure techniques that imitate the human intuition, such as the well known \emph{dynamic time warping} (DTW) and the \emph{earth mover's distance} (EMD) or, may took more than few hours on an average personal computer to find the most similar object in a large dataset to a query object.
Therefore, to reduce the time needed for searching a shape within a large sample set, it is desirable to make as few as possible distance calculations to candidates.
In this case sub-linear distance calculations is preferable.
This paper we propose technique that workout the these issues and demonstrate a high performance letters classification. 

The rest of the paper is organized is follows. 
In Section \ref{sec:related_work} we mention related works. 
The character classifier is described in detail in Section \ref{sec:approach}. 
Demonstrating and discussing the results is done in Section \ref{sec:results_analysis}

\section{Related Work}
\label{sec:related_work}

Literature in cursive handwriting recognition has established two main approaches: the analytic approach, which involves segmenting words into individual characters and then classifying each character\cite{abdulla2008off, sari2002off, Dinges2011}, and the holistic approach, which considers the global properties of the written text and recognizes the input word shape as a whole \cite{biadsy2011segmentation, saabni2009hierarchical}. 
The holistic approach, while having many advantages, requires the classifier to be trained over the entire dictionary, which is impractical for large dictionaries (containing more than 20,000 words) \cite{elanwar2012unconstrained}.

One of the earliest studies on On-line Arabic isolated character recognition was carried out by El-Wakil and Shoukry in 1989 \cite{el1989line}.
Since them many classification classification techniques were studied in the field of Arabic character recognition such as Artificial neural networks \cite{alijla2012oiahcr,ismail2012online}, fuzzy logic, decision trees \cite{ismail2012online}, Hidden Markov Models \cite{biadsy2006online}, and K-NN \cite{elglaly2011isolated}.
It appears that currently the $k$-nearest neighbours is among the leading techniques \cite{elglaly2011isolated}.

Linearly scanning the sample set to obtain the $k$-nearest neighbours may be an inhibitor when real-time performance is required, therefore fast $k$-NN search techniques are usually employed.
However, the key assumption for using such techniques requires that objects in the dataset lie in a metric space, i.e., the similarity measure (the distance) satisfies the triangle equality.
This assumption is not valid for many similarity measure techniques, such as EMD.
This can be partially solved by applying embedding techniques   
The embedding is required to preserve distances, in a way such that the distance in the embedding space should approximate the similarity measure of the patterns in the feature space.

The \emph{earth movers distance} (EMD) is a natural and intuitive metric to measure similarity between two histograms.
EMD was used by Saabne in \cite{saabni2013efficient} to measure similarity between shapes for recognizing and searching Arabic words. 

{The embedding used in this paper is performed using a linear time algorithm for approximating EMD, introduced in \cite{shirdhonkar2008approximate}, that uses the sum of absolute values of the weighted wavelet coefficients of the difference histogram. 
The embedding of the feature vectors to a normed space facilitate the use of fast nearest neighbours search techniques.
Finding the approximate $k$-nearest neighbours produces a short list which enables aplying expensive matching methods, such as DTW, yet keep sub-linear searching time.}

A recent survey done by Tagougui, Kherallah and Alimi \cite{tagougui2013online} reviews the status of research in the on-line Arabic handwriting recognition field. 

\section{Our Approach}
\label{sec:approach}
In the presented work, we propose an implementation for a fast and highly accurate classification technique for Arabic characters required in real-time script recognition systems, as the one described in \cite{kour2014real}.
In Figure \ref{fig:letters_classifier_learning_flow} we give a high level flow visualization of our letters classifier.
The classifier receives a sequence of points $S=\{p_{i}\}_{i=1}^{n}$ representing the letter trajectory and a letter position $\phi \in \{Ini, Mid, Fin, Iso\}$.
The query sequence goes through  5 stages: preprocessing, feature extraction, embedding to $L_1$, dimensionality reduction and then classification. 
Eventually the classifier outputs the $k$ nearest neighbours as scoring which indicates the similarity measure between the sequence and the candidate. 
The labelling of the candidates are then determine the query sequence classification.
In the coming Sub sections we will discuss each stage.


\begin{figure}
\centering
\includegraphics[width=1\columnwidth]{./figures/letters_classifier_learning_flow}       
\caption{High level diagram of the classifier learning flow for a letter position.}
\label{fig:letters_classifier_learning_flow}
\end{figure}
 
\subsection{Preprocessing}
Digitizers tend to generate a jagged and non-uniform sampling of the trajectory scribed on their surface
Preprocessing operations are usually needed to impose certain uniform structure on the data, to comply with the input structure required for the proper operation of the subsequent parts of the system \cite{al2011online}. 
In the current work, preprocessing consists of three methods: \emph{normalization}, \emph{noise elimination} and then \emph{re-sampling}.

Size normalization is performed to achieve a uniform size of the bounding box surrounding the pattern so that it will fit into a $[0,1]\times[0,1]$ bounding box without affecting the original aspect ratio. 

Then, the \emph{Douglas-Peucker Polyline Simplification algorithm} \cite{douglas1973algorithms} is employed for eliminating points duplication and inadequacies caused by hand vibrations. 
It reduces the number of vertices in a piecewise linear curve, given a preset tolerance parameter $\varepsilon$, and outputs a simplified curve, that consists of a subset of the points that defined the original curve.
In this work the tolerance parameter $\varepsilon$ was empirically set to ${1 \over 75}$.

The previous simplification process produces a highly angular, and non-uniform distribution of points along the stroke trajectory.
This stage, using splines interpolation method, aims at producing an equidistant smoothed data sequence, given a re-sampling target number of points $R$, which was set to 40. 
Given a stroke $S=\{(x_i,y_i)\}_{i=1}^{n}$, let $f_{x}(d)$ and $f_{y}(d)$ be the quadratic piecewise interpolations function of $\{x_i\}_{i=1}^{n}$ and $\{y_i\}_{i=1}^{n}$, respectively. 
$f_{x}(d)$ and $f_{y}(d)$ are functions of the coordinate value with respect to the arc-length distance from the pattern's starting point. 
Let $t_i=i\frac{L}{R}$ for $i=0,...,R$ where L is the arc-length of the pattern.
The re-sampled sequence is given as follows:
\begin{equation}
\widehat{S}=\{(f_x(t_i),f_y(t_i))\}_{i=1}^{R}
\end{equation}

Figure \ref{fig:before_after_preprocessing} show the resulting sequence after applying the preprocessing method on a sequence of the letter \RL{b}. 

\begin{figure}
	\centering
        \subfloat[]{
            \label{fig:preprocessing_orig}
            \includegraphics[width=0.4\columnwidth]{./figures/preprocessing_orig}
        }
        \subfloat[]{
           \label{fig:preprocessing_norm}
           \includegraphics[width=0.4\columnwidth]{./figures/preprocessing_norm}
        }  \\
        \subfloat[]{
            \label{fig:preprocessing_simpl}
            \includegraphics[width=0.4\columnwidth]{./figures/preprocessing_simpl}
        }
        \subfloat[]{
           \label{fig:preprocessing_resamp}
           \includegraphics[width=0.4\columnwidth]{./figures/preprocessing_resamp}
        }       
    \caption{A sample of the letter \RL{b} before preprocessing (a); after normalization (b); after noise elimination (c) and after re-sampling (d).}
   \label{fig:before_after_preprocessing}
\end{figure}

\subsection{Feature Extraction}
Two shape descriptors were employed in this work, the \emph{Shape Context} \cite{belongie2002shape} proposed by Belongie et al., and the Multi-Angular descriptor (MAD) that was proposed by Saabni in \cite{saabni2013multi}.
The Shape context is a point matching approach intended to be a way of describing shapes that allows for measuring shape similarity and the recovering of point correspondences. 
It describes the distribution of all points in the shape with respect to each single point on the shape.
The shape context descriptor of each single point on the shape computes the histogram of relative polar coordinate and have proved to be one of the efficient features for matching shapes.
This distribution over relative positions is robust and compact, yet highly discriminative descriptor

The other shape descriptor used in this thesis is the Multi Angular Descriptor (MAD) that was proposed by Saabni in \cite{saabni2013multi}. 
It captures the angular view to multi resolution rings in different heights. 
The shape is treated as a two dimensional set of points and the different rings are upper view points from rings around the shape centroid with different sizes and heights. 
To enables scale and translation invariance, the sizes and heights of these rings are calculated using the diameter and centroid of the shape.

\subsection{The Earth Mover's Distance Embedding}
Histogram based descriptors, such as the Shape Context, are in many cases compared using a bin-wise dissimilarity techniques such as the Euclidean distance or the $\chi^2$ statistic.
While such dissimilarity measures can be computed very fast, they usually fail to consider local and global small variations. 
These variations, which would be perceived as minor by a human, may result in a large dissimilarity values between two histograms. 
Generally speaking, the distance between two histograms can be viewed as a special case of the well-known \emph{transportation problem}, a.k.a the Monge-Kantorovich problem \cite{rachev1985monge}.
The \emph{Earth Mover's Distance} (EMD), introduced by Rubner et al. in \cite{rubner2000earth}, is a solution to the transportation problem which can be used as a measure of the dissimilarity between histograms. 

Given two distributions, one can be seen as piles of sand and the other as a collection of holes. 
The EMD measures the least word needed to fill the holes with the sand, where a unit of work corresponds to the transporting a unit of sand from one pile to the hole depending of their distance.
EMD was experimentally verified to capture well the perceptual notion of a difference between images and is commonly used in content-based image retrieval to compute distances between the color histograms of two digital images \cite{grauman2004fast}.

The major hurdle in using EMD is its $O\left( {{N^3}\log N} \right)$ computational complexity (for an $N$-bin histogram). 
The complexity is magnified when the task is to search for similar shapes (nearest neighbours) in a large database. 
In this case, a linear scan of the database would require computing a comparison of super-polynomial complexity for each database member against the query shape \cite{grauman2004fast}. 
Greatly reducing the EMD calculation time can be achieved using metric approximation technique.  

Several approximation algorithms have been proposed to speed-up the computation of EMD. 
Indyk and Thaper \cite{indyk2003fast} proposed a technique for embedding the un-normed EMD metric into the $L_1$ space so that the EMD distance between the two objects is comparable to the Manhattan distance between the two points which represent the embedding of the two objects.
In a subsequent work done by Shirdhonkar and Jacobs \cite{shirdhonkar2008approximate}, the authors proposed a method for approximating the EMD between two low dimensional histograms using the weighted wavelet coefficients of the difference histogram. 
The approximation is done by transforming the histograms into the $L_1$ space so that the distance between the two vectors in the wavelet domain is the EMD approximation. 
Similarly to the embedding proposed by Indyk and Thaper, the EMD distance between two histograms is done by calculating the $L_1$ norm of the coefficients vector of the embedding as given in Equation \ref{eq:emd_embedding}.

\begin{equation}
d(p)_{wemd}= \sum\limits_{\lambda} 2^{-j(1+n/2)}|p_{\lambda}|
\label{eq:emd_embedding}
\end{equation}
where $p$ is the n-dimensional difference histogram and $p_{\lambda}$ is the wavelet transform coefficients. 
The index $\lambda$ includes both shifts and the scale j.
Intuitively, the wavelet transform splits up the difference histogram according to scale and location where each coefficient represents the solution to the EMD subproblem. 
For a single wavelet, the mass to be moved is proportional to the volume of $|\psi_j(x)|$, i.e., to $2^{-jn/2}$ and the distance to be travelled is proportional to the span of the wavelet, i.e $2^{-j}$. The sum of all distances is an approximation to EMD, as formally defined in Equation \ref{eq:emd_embedding}. 
This can be viewed as similar to the way packages are shipped over large distances. 
The route is broken into several pieces which are large and small distances. 
Packages from nearby places are merged at the end of the short distance route piece to travel together. 
Then another merge is done of packages from the entire country to be shipped together to the destination country. 
The sum of the distances travelled is an approximation to the actual distance.

\subsection{Dimensionality Reduction}
Dimensionality reduction methods aims to reducing the number of variables taken into consideration for the reason that high-dimensional data present many practical complications and performance issues. 
Ideally, the reduced representation should have a dimensionality that corresponds to the intrinsic dimensionality of the data \cite{van2009dimensionality} which is the minimum number of parameters needed to account for.
The need for employing dimensionality reduction in this work emerged from the sparse vectors produces by the EMD embedding of the feature vectors into the wavelet coefficient domain.
For example, when employed the shape context descriptor the embedding produced lengthy sparse vectors in $R^{3946}$. 

PCA is unsupervised technique that transforms the data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (the first principal component), the second greatest variance on the second coordinate, and so on.
The number of principle components to consider can be calculated easily using the sum of the eigenvalues of the covariance matrix \emph{TODO: add reference}.
The major drawback of PCA is that it is an unsupervised technique, namely, it does not takes into consideration the labeling information of the data and heavily rely on the assumption that most information is contained in those directions where input data variance is maximum.
Thus PCA is appropriate in cases when the samples are taken from a single class.
On the contrary, \emph{linear discrimination analysis} (LDA) \cite{fisher1936use} is a supervised technique performing dimensionality reduction while preserving as much of the class discriminatory information as possible. 

PCA followed by LDA is a commonly used technique in fields where the feature space is highly dimensional such as image recognition \ref{yu2001direct,yang2003can}
In this work the samples are projected to the PCA space where the target dimensionality, i.e., the number of principal components taken into consideration, is the minimal to achieve data preservation rate of 99\%.
As seen in table \ref{table:dr_dimensions_results}, the dimensionality was reduced by PCA in two orders of magnitude.

Since there is a large variation in handwritten letters in the Arabic script, trying to group different perceptual shapes in a single class would negatively effect the dimensionality reduction process.
Thus, before applying LDA, each class was partitioned into four clusters using $L_1$-k-medoids algorithm and received a different sub-labelling. 
This sub-labeling was then used by the LDA where the target number of dimensions determined using \emph{Intrinsic dimensionality estimation} method described in\cite{van2007introduction}.

\begin{table}
\centering
\begin{tabular}{ | c | c | c | c |}
\hline
Letter position & Number of samples & PCA & PCA+LDA\\
\hline                 
  Iso & 1372 & 29 & 7 \\ 
  \hline
  Ini & 1405 & 39 & 9 \\ 
  \hline
  Mid & 1196 & 36 & 8 \\ 
  \hline
  Fin & 1629 & 27 & 7 \\ 
  \hline
\end{tabular}
\caption{Dimensionality Reduction results.The original data set dimensionality is 3946. The PCA column shows dimensionality of the data after applying PCA. The PCA+LDA column shows the dimensionality of the data after applying LDA subsequently after LDA as described.}
\label{table:dr_dimensions_results} 
\end{table}

\subsection{Fast k-nearest Neighbour Retrieval}

In the case of k-d tree, it requires the $L_p$ norm for its operation, which is even a more specific demand.


The embedding of the sample set into a normed space facilitate the usage of Approximate k-NN techniques such as k-d trees \cite{bentley1975multidimensional}, and Locality Sensitive Hashing (LSH) \cite{gionis1999similarity} which have been successfully applied on a variety of fast similarity retrieval problems. 

$k$-d tree aims at solving the problem of searching $k$-NN in a large set of multi-dimensional points by building a data structure based on the set of reference points.
The $k$-d tree construction time can be ignored since it is performed Off-line.

While the worst case scenario complexity is $O(N)$, the expected complexity of a single $k$-NN query using k-d tree is $O(\log N)$.

\subsection{Candidates Scoring}
For each subsequence, the recognition system returns a set of K potential letters candidates, with their resemblance scoring. 
In the current implementation we consider only the candidate with the best (minimal) scoring, however, any function taking into consideration the scoring returned by the classifier for all the candidate can be used to determine the final scoring of the cell.
Once the set of candidates determined by the closeness found by the k-d tree based on the coefficients vector in the $L_1$ space. The scoring is performed as follows:

The scoring of the candidates is done by averaging the score of the $L_1$ distance between the query object and the candidate in the sample set, and the score of the DTW distance between the feature vector and the 
Let us the denote the feature vector of the query object by $q$ and the feature vector the candidate $i$ by $c_i$.
\begin{equation}
Scoring(c_1)=\frac{EMD_{approx.}(q,c_i)+DTW(q,c_i)}{2} 
\end{equation}

This gives an equal weigth to the DTW and the EMD approximation.
We have found the constraint DTW performs using the unconstrained DTW.
The weights given for each similarity measure technique was determined empirically.

\emph{check other scoring techniques, such as DTW alone, EMD alone, DTW+Approximation}


\section{Results and Analysis}
\label{sec:results_analysis}

%\begin{enumerate}
%\item compare with other letter classifiers or digits.
%\item compare the performance of classification without using our approach.
%\item show the affect of the number of the sample set on the classification rate. If we show that the larger the sample set, the less is the recognition rate, it will contradict what wwe have said in the introduction about the need in a large training set.
%\end{enumerate}


The character samples used in this work are taken form the ADAB database,\cite{el2009icdar}, which is a de-facto standard in the field of on-line Arabic handwriting recognition. 
It is freely available and consists of more than 20k Arabic handwritten words (937 Tunisian town/village names) scribed by more than 170 different writers. 
A manual segmentation of the samples in the database was needed to extract the letters samples.
Using a public and not a self collected samples gives our results a further firmness.
We've used a small validation set distributed as follows: 1404 Initial form samples, 1195 Medial, 1628 Final and 1371 Isolated letter samples. 
The extracted  set was used for both training the system and testing the performance of the classifier.
Measuring the system accuracy and performance in all the experiments below were done using 10-fold cross-validation for each position letter database (Ini, Mid, Fin and Iso) independently.
The values calculated for the accuracy measures in the following experiments are done by weighted average of the performance of each position.

The results shown in table \ref{table:features_comparison} compares the two feature extraction techniques used in this work and compare it to the results obtained when none is used, namely, using the letter sequence as the vectors in the feature space.
It can be seen that the Sequence, i.e. using no feature extraction, have achieved almost the same accuracy results as when using SC.
Although at first glance it can make doubting the necessity the feature extraction stage, we will see in later that the use of SC improved the segmentation results considerably when performing the on-line segmentation and recognition.
It boosted the SR from 75\% to 81\% and the RR from 72\% to 76\%.
This may be explained by the fact that although the same accuracy results in letters classification, the scoring given to candidates is more accurate.


\emph{notes: Try to tune the MAD or else drop it.}
\begin{table}
\centering
\begin{tabular}{ | c | c | c | c | c |}
\hline
Feature & Precision & Recall & F-measure \\
\hline                 
  Sequence  (None) &  0.87 & 0.87 & 0.867 \\
  \hline
  Shape Context & 0.86 & 0.86 & 0.86 \\
  \hline
  MAD & 0.77 & 0.78 & 0.77 \\
  \hline
\end{tabular}
\caption{Features comparison. }
\label{table:features_comparison} 
\end{table}

The next series of experiments aims to quantifying the contribution and importance of each stage in the classification required us to conduct a series of experiments.
In each experiment, we skip a stage in the recognition system and then measure the classifier performance.
The three main stages in the classifier are: 1. the feature extraction stage; 2. the EMD embedding; and 3. the Dimensionality reduction.
As detailed in Section \ref{sec:dr} the role of the DR process is to reduce the number of variables taken into consideration by the classifier in order to avoid problem caused by the "curse of dimensionality", without affecting much the system accuracy.
Table \ref{table:dr_embedding_comparison} shows the results of the experiment that uses SC as the feature.
The results in Table \ref{table:dr_embedding_comparison} shows that the dimensionality reduction technique implemented in this work almost does not affect the system performance.


\begin{table}
\centering
\begin{tabular}{ | c | c | c | c | c | c |}
  \hline
  Experiment & Precision & Recall & F-measure\\
  \hline                 
  Baseline &  0.87 & 0.87 & 0.867 \\
  \hline
  No DR & 0.91 & 0.9 & 0.90 \\
  \hline
  No embedding & 0.89 & 0.89 & 0.89 \\
  \hline
\end{tabular}
\caption{Different Experiments}
\label{table:dr_embedding_comparison} 
\end{table}

\subsubsection{Sample set size and distribution}
The letters is our training set are extracted from a database with a limited words diversity, thus, the distribution of the samples between the different classes is imbalanced. 
On one hand, it can be regarded as an advantage; since, the training set distribution reflects the a-priory probability of a letter appearance in the test set. 
On the other hand, a highly imbalanced training set is known to negatively affect many classification algorithms.
In the following experiment, we measure the effect of a large and imbalanced training set on the WP segmentation and recognition rates. 
It is done by gradually increasing the maximal allowed number of samples per class (letter and position).
 
The graph in Figure \ref{fig:num_letter_impact} shows convergence of the system's performance when the maximal number of samples is larger than 200 per class. 
Nevertheless, a miniature degradation is apparent, which is caused, probably, due to the increasing imbalance in the distribution of the training set.
In addition, it is evident that the recognition rate (RR) is more sensitive to small training set than the segmentation rate (SR).

\section{Summary and Future Work}

\section*{Acknowledgment}
The authors would like to thank Professor Dana Ron, from the Tel-Aviv University, for her invaluable help in this research and her insightful comments on this paper. This work has been supported in part by the German Research Foundation under grant no. FI 1494/3-2.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibliography}

\end{document}


